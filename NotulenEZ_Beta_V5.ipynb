{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d0ffced-74ac-4149-b4cf-c3aa4059b290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Harus mengembalikan True jika GPU tersedia\n",
    "print(torch.cuda.current_device())  # Menampilkan ID GPU saat ini\n",
    "print(torch.cuda.get_device_name(0))  # Nama GPU pertama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4241b90d-452d-4636-8757-da801e5371ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FFmpeg ditemukan di: C:\\Program Files\\ArcGIS\\Pro\\bin\\Python\\envs\\arcgispro-py3\\Library\\bin\\ffmpeg.exe\n"
     ]
    }
   ],
   "source": [
    "from pydub.utils import which\n",
    "\n",
    "ffmpeg_path = which(\"ffmpeg\")\n",
    "if ffmpeg_path:\n",
    "    print(f\"‚úÖ FFmpeg ditemukan di: {ffmpeg_path}\")\n",
    "else:\n",
    "    print(\"‚ùå FFmpeg tidak ditemukan, pastikan sudah terinstal dan ada di PATH.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68e563c3-43bb-47cf-8077-e407716e92f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg\n"
     ]
    }
   ],
   "source": [
    "from pydub.utils import get_encoder_name\n",
    "print(get_encoder_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d91c64f-2f30-47b3-819a-cc19b57c5527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyannote.audio installed successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyannote.audio import Pipeline\n",
    "print(\"pyannote.audio installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "632b61d1-8311-4ace-89c6-21864131957e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File dipilih: D:/2024/MODEL AI/NOTULEN AI/TEST HASIL RAPAT/AUDIO_TEST_CLIP.WAV\n",
      "üîÑ Memproses diarization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "C:\\Users\\zavaa\\anaconda3\\envs\\zava_env\\Lib\\inspect.py:988: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  if ismodule(module) and hasattr(module, '__file__'):\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.0.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint C:\\Users\\zavaa\\.cache\\torch\\pyannote\\models--pyannote--segmentation\\snapshots\\c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b\\pytorch_model.bin`\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.6.0+cu118. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zavaa\\AppData\\Roaming\\Python\\Python311\\site-packages\\speechbrain\\utils\\fetching.py:151: UserWarning: Using SYMLINK strategy on Windows for fetching potentially requires elevated privileges and is not recommended. See `LocalStrategy` documentation.\n",
      "  warnings.warn(\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "C:\\Users\\zavaa\\AppData\\Roaming\\Python\\Python311\\site-packages\\speechbrain\\utils\\autocast.py:68: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "C:\\Users\\zavaa\\AppData\\Roaming\\Python\\Python311\\site-packages\\speechbrain\\utils\\parameter_transfer.py:234: UserWarning: Requested Pretrainer collection using symlinks on Windows. This might not work; see `LocalStrategy` documentation. Consider unsetting `collect_in` in Pretrainer to avoid symlinking altogether.\n",
      "  warnings.warn(\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Memulai transkripsi...\n",
      "‚úÖ Transkripsi selesai, disimpan di D:/2024/MODEL AI/NOTULEN AI/TEST HASIL RAPAT/AUDIO_TEST_CLIP_transcript.txt\n",
      "üîÑ Memuat model BART untuk rangkuman...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Merangkum teks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but your input_length is only 205. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=102)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Rangkuman disimpan di D:/2024/MODEL AI/NOTULEN AI/TEST HASIL RAPAT/AUDIO_TEST_CLIP_summary.docx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import whisper\n",
    "import torch\n",
    "import warnings\n",
    "import subprocess\n",
    "from collections import defaultdict\n",
    "from transformers import pipeline\n",
    "from docx import Document\n",
    "from tkinter import Tk, filedialog\n",
    "from tqdm import tqdm\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"FP16 is not supported on CPU\")\n",
    "\n",
    "# Path FFMPEG\n",
    "FFMPEG_PATH = r\"C:\\Users\\zavaa\\OneDrive\\Desktop\\ffmpeg-2025-01-30-git-1911a6ec26-full_build\\bin\\ffmpeg.exe\"\n",
    "SUPPORTED_FORMATS = {\"mp3\", \"m4a\", \"ogg\", \"flac\", \"aac\", \"wav\"}\n",
    "\n",
    "# Hugging Face API Token untuk Pyannote\n",
    "HF_TOKEN = \"Input HF token disini\"\n",
    "\n",
    "def check_ffmpeg():\n",
    "    try:\n",
    "        subprocess.run([FFMPEG_PATH, \"-version\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "if not check_ffmpeg():\n",
    "    print(\"‚ùå FFmpeg tidak ditemukan!\")\n",
    "    exit(1)\n",
    "\n",
    "def select_audio_file():\n",
    "    root = Tk()\n",
    "    root.withdraw()\n",
    "    return filedialog.askopenfilename(title=\"Pilih file audio\", filetypes=[(\"Audio Files\", \"*.m4a;*.mp3;*.wav;*.flac;*.ogg;*.aac\")])\n",
    "\n",
    "def convert_audio_to_wav(input_file, output_file=\"temp_audio.wav\"):\n",
    "    if not os.path.exists(input_file):\n",
    "        print(\"‚ùå File tidak ditemukan:\", input_file)\n",
    "        return None\n",
    "    \n",
    "    file_extension = os.path.splitext(input_file)[-1].lower()[1:]\n",
    "    if file_extension == \"wav\":\n",
    "        return input_file\n",
    "    \n",
    "    if file_extension not in SUPPORTED_FORMATS:\n",
    "        print(f\"‚ùå Format {file_extension} tidak didukung!\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        print(f\"üîÑ Mengonversi {file_extension} ke WAV dengan FFmpeg...\")\n",
    "        command = [FFMPEG_PATH, \"-i\", input_file, \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-y\", output_file]\n",
    "        subprocess.run(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)\n",
    "        return output_file\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(\"‚ùå FFmpeg gagal mengonversi!\")\n",
    "        return None\n",
    "\n",
    "def diarize_audio(file_path):\n",
    "    print(\"üîÑ Memproses diarization...\")\n",
    "    pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=HF_TOKEN)\n",
    "    diarization = pipeline(file_path)\n",
    "    return diarization\n",
    "\n",
    "def transcribe_audio(file_path, diarization):\n",
    "    converted_path = convert_audio_to_wav(file_path)\n",
    "    if not converted_path:\n",
    "        return \"\"\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = whisper.load_model(\"medium\").to(device)\n",
    "    \n",
    "    print(\"üîÑ Memulai transkripsi...\")\n",
    "    result = model.transcribe(converted_path, language=None, fp16=(device == \"cuda\"))\n",
    "    \n",
    "    if converted_path == \"temp_audio.wav\":\n",
    "        os.remove(converted_path)\n",
    "    \n",
    "    speaker_texts = defaultdict(list)\n",
    "    for segment in result[\"segments\"]:\n",
    "        start_time = segment[\"start\"]\n",
    "        end_time = segment[\"end\"]\n",
    "        text = segment[\"text\"]\n",
    "        \n",
    "        speaker = \"Unknown\"\n",
    "        for turn, _, spk in diarization.itertracks(yield_label=True):\n",
    "            if turn.start <= start_time and turn.end >= end_time:\n",
    "                speaker = f\"Speaker {spk}\"\n",
    "                break\n",
    "        \n",
    "        speaker_texts[speaker].append(f\"[{start_time:.2f} - {end_time:.2f}] {text}\")\n",
    "    \n",
    "    return speaker_texts\n",
    "\n",
    "def save_text_to_file(transcript, output_file):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for speaker, texts in transcript.items():\n",
    "            f.write(f\"{speaker}\\n\")\n",
    "            f.write(\"\\n\".join(texts) + \"\\n\\n\")\n",
    "\n",
    "def summarize_text(transcript):\n",
    "    print(\"üîÑ Memuat model BART untuk rangkuman...\")\n",
    "    device = 0 if torch.cuda.is_available() else -1\n",
    "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=device)\n",
    "    \n",
    "    print(\"üîÑ Merangkum teks...\")\n",
    "    full_text = \" \".join([\" \".join(texts) for texts in transcript.values()])\n",
    "    summary_chunks = [summarizer(full_text[i:i+1024], max_length=250, min_length=100, do_sample=False, truncation=True)[0]['summary_text'] for i in range(0, len(full_text), 1024)]\n",
    "    \n",
    "    summary = \"\\n- \" + \"\\n- \".join(summary_chunks)\n",
    "    return summary.strip()\n",
    "\n",
    "def save_summary_to_docx(summary, output_file):\n",
    "    doc = Document()\n",
    "    doc.add_heading(\"Summary\", level=1)\n",
    "    doc.add_paragraph(summary)\n",
    "    doc.save(output_file)\n",
    "\n",
    "def main():\n",
    "    audio_file = select_audio_file()\n",
    "    if not audio_file:\n",
    "        print(\"‚ùå Tidak ada file yang dipilih.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"‚úÖ File dipilih: {audio_file}\")\n",
    "    diarization = diarize_audio(audio_file)\n",
    "    transcript = transcribe_audio(audio_file, diarization)\n",
    "    if not transcript:\n",
    "        print(\"‚ùå Transkripsi gagal.\")\n",
    "        return\n",
    "    \n",
    "    transcript_file = os.path.splitext(audio_file)[0] + \"_transcript.txt\"\n",
    "    save_text_to_file(transcript, transcript_file)\n",
    "    print(f\"‚úÖ Transkripsi selesai, disimpan di {transcript_file}\")\n",
    "    \n",
    "    summary = summarize_text(transcript)\n",
    "    summary_file = os.path.splitext(audio_file)[0] + \"_summary.docx\"\n",
    "    save_summary_to_docx(summary, summary_file)\n",
    "    print(f\"‚úÖ Rangkuman disimpan di {summary_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (zava_env)",
   "language": "python",
   "name": "zava_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
